### Some less cited papers but worth the read I think.

- [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820)(The famous `super-convergence` paper)
- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)

- [NALU](https://arxiv.org/abs/1808.00508)

- [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)

- [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)

These are some of my personal favorties. I won't claim that I understand the complete math underlying each of the above approaches, but 
yes, I do understand them on an intuitive level. 
